---
layout: post
title: "Day 39 â€“ Training Double DQN Agent"
date: 2025-07-18
author: Holy Agyei
permalink: /day39.html
tags: ["Double DQN", "Dueling Network", "Training"]

what_i_learned: |
  Today, I worked on training a Double DQN agent with a Dueling Network architecture using the T1D Hypertension Environment logic. 
  The dataset has a shape of (1039, 9) with columns including 'Patient_ID', 'Alanine transaminase (GPT)', 'Creatinine', 'Glucose', 'Potassium', 'Sodium', 'Sex', and 'Age'. 
  Training started on the CPU, and the agent is learning to optimize rewards. 
  Episodes showed improvements in rewards over time, with some episodes achieving new best rewards. 
  The epsilon value is decaying as training progresses, indicating a shift towards exploitation.

blockers: |
  It was hard to make the training run smoothly, and it's taking high computational energy and time. 
  Training a Double DQN agent with a Dueling Network architecture can be computationally intensive. 
  The complexity of the environment and the chosen hyperparameters might be contributing to the high computational requirements. 
  We're monitoring the average rewards and epsilon values to ensure proper learning despite these challenges. 
  Optimizing computational efficiency might be necessary for larger-scale training or more complex environments.

reflection: |
  I'm pleased with the progress of the training so far. 
  The Double DQN agent with a Dueling Network architecture seems to be learning effectively in the T1D Hypertension Environment. 
  The improvements in rewards over episodes indicate that the agent is adapting to the environment. 
  We'll continue training and monitoring the agent's performance to ensure it learns optimal policies. 
  Analyzing the results and adjusting hyperparameters as needed will be crucial for achieving the best outcomes.
---
